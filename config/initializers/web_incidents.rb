WEB_INCIDENTS = [
  "JWT-based SSO accepted tokens whose role claims were still valid while the session store had already marked the browser session as terminated. A 4-minute clock skew between two API nodes and the IdP meant exp and nbf were evaluated differently per service. Edge caches stripped Cache-Control: private and reused the Authorization header across requests when connection reuse hit, so downstream services intermittently received a bearer token that had been rotated. Internal gRPC calls forwarded only the embedded role claim and never revalidated the session ID against the session store, so replayed tokens crossed trust boundaries into billing and support tooling. Security noticed that refresh-token rotation succeeded but the access-token validator relied on local wall clocks and a warm cache of JWKs that lagged revocation by ~10 minutes. Incidents clustered around deployments that restarted the validator pods, which repopulated the key cache and extended the vulnerable window.",
  "A CDN layer cached admin API responses because the origin forgot to set Cache-Control: no-store, private and Vary: Authorization. During a brownout, the CDN shield returned a 200 cached body for a privileged GET on /admin/users/:id to non-admin clients who hit the same edge POP and header set. Downstream auditors found interleaved cache keys without the Authorization header in the vary set, and the surrogate cache key incorrectly normalized query parameters, collapsing /admin/users/123?include=emails and /admin/users/123?include=roles. Since the admin SPA reused fetch URLs across views, sensitive payloads appeared in regular user sessions for several minutes after an admin visited the same user record. The origin logs showed minimal load because the responses were served from edge, delaying detection until support tickets arrived about 'seeing another person’s email addresses in the dashboard.'",
  "Kafka compaction was enabled for an account-state topic, but producers switched from idempotent keys (account_id) to per-request UUIDs during a refactor. The compactor retained every message, ballooning partitions and pushing consumer lag past the retention horizon. A canary consumer with enable.auto.commit=true committed offsets before performing the write to the read model; when nodes restarted, they skipped unprocessed messages that had already been committed. Failover added duplicates from the new idempotent producer (transactional.id) colliding with the old non-idempotent stream, yielding both missing updates and replayed events. Recovery was complicated by compacted tombstones that hid the order in which corrections were applied; the only source of truth became the raw change feed, which was incomplete in cold storage because the tiered storage writer had been disabled weeks earlier.",
  "A blue/green deploy migrated orders by creating orders_v2, backfilling, and then swapping a view named orders to point at the new table. Long-running transactions from report exports had started before the view swap and continued to read the old orders_v1 snapshot for hours. Writers updated orders_v2, readers randomly saw orders_v1 until their transactions completed, and a background cleanup job dropped orders_v1 while one replica was still applying WAL from the old table. The ORM connection pool cached the old table OIDs, generating 'relation does not exist' errors only on that replica. Support escalated when customers noticed missing line items that later reappeared; the root cause was a migration plan that assumed statement-level atomicity while the workload contained multi-minute transactions and replica lag.",
  "A feature flag rollout used a percentage-based rule keyed by user_id, but background workers processed webhooks with service_account_id and defaulted to the flag’s bootstrap value (true) when the evaluation key was missing. During a partial outage, the flag service returned 429 to workers; the SDK treated errors as true due to a mis-set fallbackVariation. As a result, a data deletion feature activated for all tenants in workers while remaining off in the frontend, causing divergent behavior and irreversible deletions. The kill switch took effect for interactive traffic only; batch jobs continued using stale SDK caches for 5 minutes, producing a second wave of damage. The postmortem revealed we never documented flag evaluation keys per traffic class or enforced parity between interactive and batch contexts.",
  "S3 event-driven processing missed entire hours of exports because the bucket notification was scoped to the prefix daily/ while the exporter wrote to daily/2025-11-02/ (note the trailing slash difference from the configured rule). The exporter retried uploads on 500s and eventually succeeded, but the Lambda trigger never fired. A compensating job polled the bucket using eventual-listing semantics; list results lagged by minutes and occasionally omitted keys during high-tenant churn, so the poller incorrectly marked the day complete. Finance reconciliations noticed missing statements two weeks later when downstream ledger diffs didn’t match the processor’s reports. Adding versioned listings and S3 Inventory would have surfaced the gap, but the team assumed the event system was authoritative and removed the nightly inventory job to save cost.",
  "A shared rate limiter keyed by client_ip throttled an entire office when their traffic emerged from one NAT egress after a firewall upgrade. The microservice advertised 'per-user' limits, but the proxy layer applied limits before authentication and forwarded only X-Forwarded-For with the NAT address. During an incident, burst retries escalated the problem: the limiter tripped at the edge, clients backed off inconsistently, and mobile users on cellular routed differently and continued to succeed, creating the perception of randomness. Telemetry suggested the ‘abuser’ was a single IP; support banned it, taking out the whole office during quarter-end close. The remediation required moving to a post-auth key (tenant_id + user_id + grant_type) and adding a shadow limiter to observe the new cardinality before cutover.",
  "mTLS rotation used Envoy SDS to hot-swap certificates, but one service used a bootstrap static secret file and applied reloads only on process restart. As CA rotation advanced, peers started requiring the intermediate that the static node didn’t trust; mutual authentication succeeded in one direction (client→server) but failed in the reverse for callbacks. Circuit breakers tripped, the retry budget was consumed, and the orchestrator interpreted 503s as unhealthy, rescheduling pods in a tight loop. Because readiness checks were unauthenticated HTTP, the deployment rolled forward despite broken mTLS, defeating the canary. The incident uncovered an assumption that 'mTLS is symmetric,' while operational reality was split: one half was dynamic with SDS, the other pinned to files on disk baked into an older base image.",
  "A proxy added request/response logging at INFO level during a high-severity investigation and captured OAuth refresh tokens in headers. Logs streamed to a shared analytics account with 30-day retention and were readable by a contractor role used for BI dashboards. Attackers who phished those credentials weeks later replayed the refresh tokens to mint long-lived access tokens, bypassing MFA because refresh grants were not bound to device posture. Downstream systems trusted the ID token claims and didn’t enforce continuous re-auth on risky actions; a batch delete was executed in billing using that path. We lacked audience and sender constraints on refresh grants, and we never flipped the 'rotate on use' flag in the auth server, so exposed tokens remained valid for the entire retention period.",
  "Redis used AOF with appendfsync=everysec, but disk saturation caused background rewrite to stall. Memory ballooned, eviction began, and session keys with volatile-lru were evicted while long-lived cart objects without TTL remained. Users appeared to be logged out randomly; in reality, their sessions disappeared under memory pressure while carts persisted. After a failover, the replica promoted with an incomplete AOF rewrite that truncated near a boundary; some sessions reappeared, others remained lost, and a subset of carts rolled back to older states. Because we considered sessions non-critical, no synthetic probes were tracking loss rates; detection came from support chat volume spikes, not alerts. Fixes included separating keyspaces by function, sizing AOF rewrite I/O, and pinning session TTLs to a dedicated Redis tier.",
  "Google Pub/Sub ordering keys were introduced to guarantee in-order processing per invoice_id, but the consumer pool size was increased without raising the per-key concurrency cap; hot keys serialized while cold keys flooded, pushing ack deadlines past visibility windows. The library auto-extended acks but the worker restarted mid-extend; unacked messages were redelivered out of order. The saga orchestrator timed out compensations and sent reversal events that then arrived before original actions. Because producers retried with identical payloads but new message IDs, the deduper in the consumer mistook them for novel work and executed side effects twice. The entire pipeline assumed ‘at least once + idempotency’, but idempotency keys were on the write models, not the side-effecting HTTP calls to vendors, causing external double charges.",
  "Kubernetes liveness probes killed pods that were still warming caches and performing migrations after a rolling deploy. Readiness succeeded prematurely because the HTTP /health endpoint returned 200 once the process started, not when dependencies were ready. Under load, liveness timeouts triggered, the scheduler rescheduled replicas onto nodes with no warmed page cache, and work oscillated across the cluster without ever stabilizing. Downstream clients observed intermittent 502s and exponential backoff magnified the thundering herd. The fix involved splitting health endpoints, lengthening initial delays, using startupProbe, and gating traffic entry on a synthetic transaction that verified DB schema and cache priming rather than process liveness.",
  "A regional routing change introduced a BGP community tag that our transit provider honored by preferring a cheaper path with higher latency. Health checks to application nodes originated from a NAT range allowed by security groups; user traffic arrived from the internet and was shaped differently. Load balancers continued marking targets healthy even as user requests timed out behind a mis-sized firewall doing stateful inspection that exhausted ephemeral ports under burst. The fleet autoscaler added nodes into the same AZ, amplifying the bottleneck. Because synthetic monitoring used the same NAT path as health checks, dashboards remained green until customer complaints arrived. Only traceroute from diverse vantage points revealed asymmetry between checkers and real traffic.",
  "A streaming pipeline used a watermark derived from DB updated_at, but an application server wrote rows with timestamps generated in the application layer where NTP drifted by ~90 seconds. On DST transition, one shard advanced an hour while another lagged; the watermark logic skipped records that appeared 'from the future.' Late data was never backfilled because backfill jobs bounded queries using the same flawed watermark windows. Search results looked stale even though the transactional store had the newest data. The fix required moving to monotonically increasing IDs generated in the database, adding late-arrival lanes, and surfacing ‘events behind watermark’ metrics that alarmed on divergence instead of throughput.",
  "Idempotency keys for payments were bound to the worker attempt UUID rather than the natural key (client-provided request_id). Gateways saw each retry as unique and settled them independently; the ledger deduped later, but webhook notifications to third parties fired for every settlement because the deduper lived in the ledger, not the webhook publisher. Marketing systems reacted to webhooks and sent multiple 'payment received' emails, while refunds initiated by support raced with delayed settlements and caused negative balances. Correctness required aligning the idempotency boundary at the edge (API) and propagating the same key through worker queues, gateway requests, and webhook emissions.",
  "Observability sampling flipped to 0% during the busiest 30 minutes because the dynamic sampler was configured to target a fixed event rate across the whole cluster. When traffic spiked, per-node budgets fell below 1 event/sec and the SDK rounded to zero. Paging relied on error-ratio alerts computed from sampled traces, not raw metrics, so critical alerts never fired. After the window passed, sampling resumed and dashboards showed a healthy system with tiny blips. We assumed logs would fill the gap, but the central log pipeline hit a shard hot-spot and dropped log lines due to backpressure with no DLQ. We now cap the minimum sampling rate per service and base paging on unsampled SLI metrics.",
  "A month-end cron expressed as 0 0 L * * (last day of month) ran twice because the platform interpreted ‘L’ in local time while the orchestrator scheduled in UTC. February’s leap-year test was skipped in staging because the schedule was copied with 'Run now' during validation. In production, the cron executed at 00:00 local time and again at 00:00 UTC, generating duplicate invoices and credit memos that then raced in the ledger. Reconciliation scripts that depended on idempotent ledger entries failed because the idempotency key factored in run_id, which differed per invocation. The incident uncovered that schedules were not pinned to a canonical zone nor simulated across calendar boundaries in CI.",
  "A cache stampede occurred when a 5-minute TTL on a hot key expired across 200 web nodes at once. Without request coalescing or dogpile locks, each node issued a heavy DB query, saturating the read replica and causing failover to the primary. The primary slowed due to checkpoint I/O, increasing latency and triggering client-side retries, which then bypassed cache entirely because error paths skipped the cache set. The TTL had been aligned to the top of the minute by an earlier optimization for cache observability, unintentionally synchronizing expiry. Fixes included jittered TTLs, single-flight guards, and rewriting error paths to set stale-while-revalidate entries instead of skipping the cache.",
  "CloudFront signed URLs were configured with a wildcard path that included /admin/ assets intended only for staff. The SPA embedded a pre-signed URL in HTML, and when users shared links, downstream recipients could fetch admin-only static JSON configuration that contained feature-flag secrets and support email tokens. The bucket policy relied on aws:Referer checks, which clients circumvented by direct fetches; some browsers cached the JSON and served it to non-authenticated sessions because cache keys didn’t include cookies. No alarm triggered because the objects were served from cache and logs didn’t reach the SIEM. We replaced signed URL scope with narrower prefixes, moved secrets out of static bundles, and enforced headers with Lambda@Edge that rejected responses lacking a user claim.",
  "A Terraform module upgrade removed create_before_destroy from a security group rule managing ephemeral ports for database connections. During apply, Terraform deleted the old rule before creating the new consolidated range; pods lost DB connections for 40 seconds mid-peak, causing leader elections, queue backlogs, and customer-visible errors. Because the ASG performed rolling restarts on connection failures, the fleet churned at the same time as Terraform, compounding the outage. A -target apply during hotfix made matters worse by skipping dependent resources that would have minimized blast radius. The fix added dependency graphs, a deploy-freeze around SG changes, and smoke tests that open persistent connections while applying infra.",
  "Backups were encrypted with a rotating KMS CMK using the alias alias/backup, but restore automation specified the old key ID directly instead of the alias. After KMS key rotation and scheduled disable of the old key, DR restores failed with AccessDeniedException: KMS key is disabled. Because the last restore test predated the rotation by months, no one noticed until a real incident required point-in-time recovery. The team assumed the alias abstraction existed everywhere; in reality, a one-off script pinned the key for speed. We remediated by forcing all consumers to resolve the alias at runtime, adding pre-rotation rehearsal restores, and setting CloudWatch alarms on decrypt denials during restore drills.",
  "Automated canary analysis computed p95 latency by merging histograms from the control and experiment over different time windows, violating percentile algebra. Under traffic spikes, the control’s 10-minute window smoothed latency while the canary’s 1-minute window spiked, triggering false rollbacks. The deploy controller oscillated between versions for an hour, exhausting retry budgets and confusing support who saw alternating behavior. Only after exporting raw latency distributions and rederiving per-version percentiles over the same interval did we realize the aggregator conflated window sizes. Remediation: align windows, compare quantile summaries per series, and gate rollbacks on multiple independent indicators (errors, saturations) instead of a single p95 comparison.",
  "Elasticsearch hit the high water mark on one node; shard allocation was throttled, and the cluster refused to allocate primaries for new indices. Writers continued to accept documents due to buffering and refresh interval behavior, but searches returned stale results because the replicas of hot indices were unassigned. Fielddata exploded when a new facet was added on a high cardinality field, spiking heap on one node only. Because alerts looked only at cluster-wide health, the partial allocation didn’t page anyone. Fixes: per-node watermarks, circuit breakers for fielddata, and rejecting writes when the target index lacks an assigned primary.",
  "A webhook consumer acknowledged messages before posting to the internal command bus, assuming retries would re-deliver on failure. When the downstream bus experienced a partition outage, acknowledged webhooks were lost with no DLQ or replay path. Vendors retried some notifications with exponential backoff, but the backoff windows didn’t align with the bus recovery, leading to gaps. Since our system of record became the vendor’s webhooks, we had no authoritative way to reconstruct missed events. The remediation introduced idempotent, transactional outbox patterns: the consumer writes to a local table and only ack’s after a successful publish, with a background replayer for the outbox.",
  "A multi-region control plane elected leaders via etcd; increased inter-region latency during a cloud provider incident caused frequent leader loss and re-election. Some components wrote with --election-timeout tuned for LAN conditions and continued to accept writes that later rolled back when the cluster reformed. Clients saw phantom successes: 200 responses followed by compensating errors minutes later. The UI showed 'saved' while the authoritative store rejected the revision due to term mismatch. The fix enforced regional write fences: all state-changing requests were directed to the local region leader with strict quorum and sticky sessions; cross-region read replicas were marked stale during instability.",
  "Paging failed because the on-call rotation in PagerDuty was correct, but the integration key used by our alert router expired months earlier when a team renamed a service. Alertmanager continued to fire webhooks that returned 403; retry logic respected backoff and never escalated to fallback email. Synthetic probes showed green because they hit the CDN, not the origin, and our SLO burn alerts were silenced for a planned maintenance window that had ended the day prior. The first human signal was a customer tweet. We now alert on the alerting system: canary alerts must page a shadow service every hour, and the router alarms if any integration returns non-2xx.",
  "A GDPR delete pipeline wrote soft deletes to the main DB and published tombstone events. A downstream CDC process mistakenly treated tombstones as ‘updates’ and republished resurrected records to the search index on compaction, undoing deletions. Weeks later, a backfill job replayed historical Kafka topics into a new environment and, due to missing deny rules, those replays leaked into production because the shared API key permitted cross-environment writes. The data subject discovered resurrected data via a subject access request. We added hard deletes for PII fields at the edge, environment-scoped keys, deny-by-default egress from backfill clusters, and end-to-end privacy tests that attempt resurrection and assert failure.",
  "Kubernetes Secrets were rotated in the cluster, but deployments mounted them as volumes without a restart policy that re-reads the content. Long-lived pods kept using the old credentials after the provider revoked the database user. Connection attempts started failing, the pool backoff increased, and the autoscaler attempted to scale up to compensate, exhausting capacity without improving success rates. Observability showed healthy secret updates in the cluster; the app layer silently held stale connections. The fix enforced checksum/secret annotations on pod templates to force rollouts on secret change, and moved high-value credentials to short-lived tokens fetched at startup with explicit refresh logic.",
  "A Lambda that wrote to an external billing API hit its 30-second timeout and retried with the same idempotency token, but the API implemented idempotency incorrectly: it treated tokens as per-connection and didn’t persist the first attempt because the response never flushed. Downstream SQS visibility was 60 seconds; inflight messages resurfaced and backlogs doubled. The vendor performed partial side effects (reserved an invoice number) on the first attempt and completed the charge on the second, resulting in gaps in our numbering and duplicates in theirs. The fix introduced a two-phase publish with an internal outbox table and required the vendor to honor idempotency regardless of connection termination.",
  "An analytics service cached experiment assignment by user_id but forgot to vary by app_version. When a feature flag changed the purchase flow only for version 8.0+, cached assignments for 7.x users were reused, contaminating the experiment and pushing control users through the new flow. The decision service wrote exposures with the intended variant but the page rendered a different variant due to the cache, creating attribution noise. Revenue dropped because the new flow lacked a payment method for older devices. The remediation included adding Vary keys to the cache, hashing inputs deterministically, and rejecting mismatched exposure/render pairs in the pipeline so corruption is visible.",
  "A Postgres logical replication slot hit max_slot_wal_keep_size, and the primary removed old WAL segments while a downstream warehouse ETL lagged. The replica continued applying until it hit the missing segment and then stopped; monitoring watched replication_lag by LSN bytes but didn’t alarm when the slot became invalid. The ETL retried by recreating the slot, which rewound the snapshot and replayed months of changes into the warehouse, doubling counts. Because we hadn’t pinned a consistent snapshot to the ETL run, backfilled rows could not be distinguished from originals. Preventive actions included setting alerts on invalidating conditions, archiving WAL to durable storage, and checkpointed ETL with high-water marks.",
  "A payment webhook handler wrote incoming events to a single partitioned table with a unique constraint on (provider, event_id). When the provider retried after 500s, their system sometimes mutated the JSON body slightly (timestamps, ordering of keys), and our ORM built a new hash as the dedupe key instead of using event_id. Under high retries, inserts bypassed the unique index and created near-duplicates that fooled downstream reconciliation into issuing refunds. The hotfix added a strict unique index on the provider’s id plus payload hash, but the root fix moved dedupe to the edge with a Redis SET keyed only by the provider’s immutable identifier, expiring after 72 hours.",
  "A Canary region used a smaller instance type with different CPU instruction sets; a native image compiled with AVX2 ran fine there but crashed in production regions on older hardware when the JIT disabled a fallback. Health checks were shallow and passed because the crash occurred only on specific endpoints that executed SIMD code paths. Traffic managers performed slow-start before full ramp, so errors appeared only after a long tail of requests hit that endpoint. Rollback succeeded but left partial data migrations that assumed the new binary’s schema. The lesson: validate binary compatibility against the oldest production hardware profile and push a synthetic workload that exercises all critical CPU code paths before traffic.",
  "A secrets manager rotated database passwords successfully, but connection pools in Go services cached credentials and never re-authenticated until restart. Midday rotation invalidated every pooled connection; the pool attempted reconnects with the old password and hit the login throttle, which cascaded to timeouts upstream. Readiness checks passed because the app returned 200 while the pool retried, so the orchestrator didn’t roll pods. The fix moved rotation to align with low-traffic windows, added pool listeners that drop caches on InvalidPassword errors, and changed readiness to run a lightweight SELECT 1 through the pool.",
  "A GraphQL gateway normalized unknown fields by dropping them silently. A downstream service added a non-nullable field to the schema and deployed out of order; the gateway stripped the field, the service defaulted it to empty, and writes succeeded with invalid invariants. Data appeared fine in the API but failed later in analytics when the missing field broke joins. Because GraphQL introspection remained green (the gateway reported the old schema), schema mismatch went unnoticed. We added schema negotiation with version pins, rejected unknown non-nullable fields at the edge, and blocked deploys when gateway and service schemas diverged beyond minor changes.",
  "An internal package registry outage returned 500s during CI, and our build system fell back to an external mirror with a different package namespace that included a typosquatted dependency. The hash matched because we verified only the top-level lockfile, not transitive dependencies. The malicious package exfiltrated environment variables during build and captured cloud credentials from CI runners. Because builds were cached, the compromised artifact propagated to staging and production. Remediation required content-addressable storage, full dependency provenance with SLSA attestations, and cutting off egress to public mirrors for private packages."
]